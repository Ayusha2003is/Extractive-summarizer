{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2c9b7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def ngram_counts(tokens, n):\n",
    "    return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "\n",
    "def rouge_n(hypothesis, reference, n=1):\n",
    "    hyp_ngrams = ngram_counts(hypothesis, n)\n",
    "    ref_ngrams = ngram_counts(reference, n)\n",
    "    overlap = len(set(hyp_ngrams) & set(ref_ngrams))\n",
    "    if len(hyp_ngrams) == 0 or len(ref_ngrams) == 0:\n",
    "        return 0.0\n",
    "    prec = overlap / len(hyp_ngrams)\n",
    "    rec = overlap / len(ref_ngrams)\n",
    "    return (2*prec*rec)/(prec+rec) if prec+rec > 0 else 0.0\n",
    "\n",
    "def lcs_length(x, y):\n",
    "    dp = np.zeros((len(x)+1, len(y)+1), dtype=int)\n",
    "    for i in range(1, len(x)+1):\n",
    "        for j in range(1, len(y)+1):\n",
    "            if x[i-1] == y[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1] + 1\n",
    "            else:\n",
    "                dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
    "    return dp[len(x)][len(y)]\n",
    "\n",
    "def rouge_l(hypothesis, reference):\n",
    "    lcs = lcs_length(hypothesis, reference)\n",
    "    prec = lcs / len(hypothesis) if hypothesis else 0\n",
    "    rec = lcs / len(reference) if reference else 0\n",
    "    return (2*prec*rec)/(prec+rec) if prec+rec > 0 else 0.0\n",
    "\n",
    "def bleu_score(hypothesis, reference, max_n=4):\n",
    "    hyp_len = len(hypothesis)\n",
    "    ref_len = len(reference)\n",
    "    if hyp_len == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    precisions = []\n",
    "    for n in range(1, max_n+1):\n",
    "        hyp_ngrams = ngram_counts(hypothesis, n)\n",
    "        ref_ngrams = ngram_counts(reference, n)\n",
    "        ref_counts = Counter(ref_ngrams)\n",
    "        overlap = 0\n",
    "        for ng in hyp_ngrams:\n",
    "            if ref_counts[ng] > 0:\n",
    "                overlap += 1\n",
    "                ref_counts[ng] -= 1\n",
    "        prec = overlap / len(hyp_ngrams) if hyp_ngrams else 0\n",
    "        precisions.append(prec if prec > 0 else 1e-9)  # smoothing\n",
    "    \n",
    "    geo_mean = np.exp(np.mean([np.log(p) for p in precisions]))\n",
    "    bp = 1.0 if hyp_len > ref_len else np.exp(1 - ref_len/hyp_len)\n",
    "    return bp * geo_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab2c291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20000 training samples\n",
      "Loaded 2500 validation samples\n",
      "Loaded 2500 test samples\n",
      "Setting up improved extractive summarizer...\n",
      "Train samples: 20000\n",
      "Val samples: 2500\n",
      "Test samples: 2500\n",
      "Building vocabulary...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "def load_data(file_name):\n",
    "    full_path = os.path.join(file_name)\n",
    "    with open(full_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [line.strip() for line in f if line.strip()]\n",
    "\n",
    "def create_dummy_data():\n",
    "    global train_src, train_tgt, val_src, val_tgt, test_src, test_tgt\n",
    "    train_src = [\n",
    "        \"This is the first sentence of the article. It's about a dog named Max. The dog is very happy and playful. Max loves to run in the park. He enjoys playing fetch with his owner. This is the last sentence about Max's adventures.\",\n",
    "        \"Another article about a different topic entirely. This one discusses recent scientific discoveries. It talks about a breakthrough in quantum computing. The discovery could revolutionize technology. Scientists are very excited about the implications. This represents a major step forward.\",\n",
    "        \"Climate change is affecting ecosystems worldwide. Many species are struggling to adapt. Arctic ice is melting at unprecedented rates. Ocean levels are rising steadily. Governments must take immediate action. The future of our planet depends on it.\",\n",
    "        \"Artificial intelligence continues to advance rapidly. Machine learning models are becoming more sophisticated. They can now perform complex tasks autonomously. However, ethical concerns remain important. Researchers emphasize responsible development. The technology holds great promise.\"\n",
    "    ]\n",
    "    train_tgt = [\n",
    "        \"Max is a happy, playful dog who loves running and playing fetch in the park.\",\n",
    "        \"Scientists made a quantum computing breakthrough that could revolutionize technology.\",\n",
    "        \"Climate change threatens ecosystems globally, requiring immediate government action.\",\n",
    "        \"AI advances rapidly with sophisticated models, but ethical development remains crucial.\"\n",
    "    ]\n",
    "    val_src = [\n",
    "        \"Space exploration has entered a new era of innovation. Private companies are launching missions. Mars colonization plans are becoming realistic. Technology advances enable longer journeys. The universe holds many secrets waiting to be discovered.\",\n",
    "        \"Renewable energy sources are becoming more efficient. Solar panels now convert more sunlight. Wind turbines generate substantial electricity. Governments invest heavily in clean technology. The transition away from fossil fuels accelerates.\"\n",
    "    ]\n",
    "    val_tgt = [\n",
    "        \"Private companies drive space exploration innovation toward realistic Mars colonization.\",\n",
    "        \"Renewable energy efficiency improves as governments accelerate the clean technology transition.\"\n",
    "    ]\n",
    "    test_src = [\n",
    "        \"Medical research has achieved remarkable breakthroughs recently. New treatments target previously incurable diseases. Gene therapy shows promising results. Clinical trials demonstrate safety and efficacy. Patients now have renewed hope for recovery.\",\n",
    "        \"Education systems worldwide are embracing digital transformation. Online learning platforms expand access. Interactive technologies engage students effectively. Teachers adapt to new methodologies. The future of education looks bright.\"\n",
    "    ]\n",
    "    test_tgt = [\n",
    "        \"Medical breakthroughs in gene therapy offer new treatments and hope for patients.\",\n",
    "        \"Digital transformation expands educational access through online platforms and interactive technologies.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Using enhanced dummy data for demonstration.\")\n",
    "\n",
    "# Load or create data\n",
    "try:\n",
    "    train_src = load_data(\"train.source.txt\")\n",
    "    train_tgt = load_data(\"train.target.txt\")\n",
    "    val_src = load_data(\"val.source.txt\")\n",
    "    val_tgt = load_data(\"val.target.txt\")\n",
    "    test_src = load_data(\"test.source.txt\")\n",
    "    test_tgt = load_data(\"test.target.txt\")\n",
    "except FileNotFoundError:\n",
    "    create_dummy_data()\n",
    "\n",
    "print(f\"Loaded {len(train_src)} training samples\")\n",
    "print(f\"Loaded {len(val_src)} validation samples\")\n",
    "print(f\"Loaded {len(test_src)} test samples\")\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self, vocab_size=10000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = {}\n",
    "        self.word_counts = Counter()\n",
    "\n",
    "    def build_vocabulary(self, texts):\n",
    "        \"\"\"Build vocabulary from text data\"\"\"\n",
    "        print(\"Building vocabulary...\")\n",
    "        \n",
    "        for text in texts:\n",
    "            words = self.tokenize(text)\n",
    "            self.word_counts.update(words)\n",
    "\n",
    "        most_common = self.word_counts.most_common(self.vocab_size - 4)  # Reserve for special tokens\n",
    "\n",
    "        # Add special tokens\n",
    "        self.word_to_idx = {\n",
    "            '<PAD>': 0,\n",
    "            '<UNK>': 1,\n",
    "            '<START>': 2,\n",
    "            '<END>': 3\n",
    "        }\n",
    "        self.idx_to_word = {0: '<PAD>', 1: '<UNK>', 2: '<START>', 3: '<END>'}\n",
    "\n",
    "        # Add regular vocabulary\n",
    "        for idx, (word, count) in enumerate(most_common, start=4):\n",
    "            self.word_to_idx[word] = idx\n",
    "            self.idx_to_word[idx] = word\n",
    "\n",
    "        print(f\"Built vocabulary with {len(self.word_to_idx)} words\")\n",
    "        return self\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Simple tokenization\"\"\"\n",
    "        text = text.lower()\n",
    "        words = re.findall(r'\\b\\w+\\b', text)\n",
    "        return words\n",
    "\n",
    "    def text_to_indices(self, text):\n",
    "        \"\"\"Convert text to list of indices\"\"\"\n",
    "        words = self.tokenize(text)\n",
    "        return [self.word_to_idx.get(word, 1) for word in words]\n",
    "\n",
    "    def indices_to_text(self, indices):\n",
    "        \"\"\"Convert indices back to text\"\"\"\n",
    "        words = [self.idx_to_word.get(idx, '<UNK>') for idx in indices \n",
    "                if idx != 0]  # Skip padding\n",
    "        return ' '.join(words)\n",
    "\n",
    "def split_into_sentences(text, max_length=50):\n",
    "    \"\"\"Split text into sentences using simple heuristics\"\"\"\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    processed_sentences = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        sent = sent.strip()\n",
    "        if len(sent) > 0:\n",
    "            words = sent.split()\n",
    "            if len(words) > max_length:\n",
    "                for i in range(0, len(words), max_length):\n",
    "                    chunk = ' '.join(words[i:i + max_length])\n",
    "                    if chunk.strip():\n",
    "                        processed_sentences.append(chunk.strip())\n",
    "            else:\n",
    "                processed_sentences.append(sent)\n",
    "    \n",
    "    return processed_sentences\n",
    "\n",
    "# Improved ROUGE and BLEU implementations\n",
    "def ngram_counts(tokens, n):\n",
    "    \"\"\"Get n-gram counts from token list\"\"\"\n",
    "    return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "\n",
    "def rouge_n(hypothesis, reference, n=1):\n",
    "    \"\"\"Compute ROUGE-N score\"\"\"\n",
    "    if not hypothesis or not reference:\n",
    "        return 0.0\n",
    "        \n",
    "    hyp_ngrams = ngram_counts(hypothesis, n)\n",
    "    ref_ngrams = ngram_counts(reference, n)\n",
    "    \n",
    "    if not hyp_ngrams or not ref_ngrams:\n",
    "        return 0.0\n",
    "        \n",
    "    overlap = len(set(hyp_ngrams) & set(ref_ngrams))\n",
    "    precision = overlap / len(hyp_ngrams)\n",
    "    recall = overlap / len(ref_ngrams)\n",
    "    \n",
    "    return (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "def lcs_length(x, y):\n",
    "    \"\"\"Compute longest common subsequence length\"\"\"\n",
    "    dp = np.zeros((len(x)+1, len(y)+1), dtype=int)\n",
    "    for i in range(1, len(x)+1):\n",
    "        for j in range(1, len(y)+1):\n",
    "            if x[i-1] == y[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1] + 1\n",
    "            else:\n",
    "                dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
    "    return dp[len(x)][len(y)]\n",
    "\n",
    "def rouge_l(hypothesis, reference):\n",
    "    \"\"\"Compute ROUGE-L score\"\"\"\n",
    "    if not hypothesis or not reference:\n",
    "        return 0.0\n",
    "        \n",
    "    lcs = lcs_length(hypothesis, reference)\n",
    "    precision = lcs / len(hypothesis)\n",
    "    recall = lcs / len(reference)\n",
    "    \n",
    "    return (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "def bleu_score(hypothesis, reference, max_n=4):\n",
    "    \"\"\"Compute BLEU score\"\"\"\n",
    "    hyp_len = len(hypothesis)\n",
    "    ref_len = len(reference)\n",
    "    \n",
    "    if hyp_len == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    precisions = []\n",
    "    for n in range(1, max_n+1):\n",
    "        hyp_ngrams = ngram_counts(hypothesis, n)\n",
    "        ref_ngrams = ngram_counts(reference, n)\n",
    "        \n",
    "        if not hyp_ngrams:\n",
    "            precisions.append(1e-9)  # smoothing\n",
    "            continue\n",
    "            \n",
    "        ref_counts = Counter(ref_ngrams)\n",
    "        overlap = 0\n",
    "        \n",
    "        for ng in hyp_ngrams:\n",
    "            if ref_counts[ng] > 0:\n",
    "                overlap += 1\n",
    "                ref_counts[ng] -= 1\n",
    "                \n",
    "        prec = overlap / len(hyp_ngrams)\n",
    "        precisions.append(prec if prec > 0 else 1e-9)\n",
    "    \n",
    "    geo_mean = np.exp(np.mean([np.log(p) for p in precisions]))\n",
    "    bp = 1.0 if hyp_len > ref_len else np.exp(1 - ref_len/hyp_len)\n",
    "    \n",
    "    return bp * geo_mean\n",
    "\n",
    "def compute_sentence_similarity(sent_tokens, summary_tokens):\n",
    "    \"\"\"Compute ROUGE-1 based similarity between sentence and summary\"\"\"\n",
    "    if not sent_tokens or not summary_tokens:\n",
    "        return 0.0\n",
    "    return rouge_n(sent_tokens, summary_tokens, n=1)\n",
    "\n",
    "def create_extractive_labels(article_text, summary_text, preprocessor, top_ratio=0.3):\n",
    "    \"\"\"Create extractive labels using ROUGE-1 similarity\"\"\"\n",
    "    sentences = split_into_sentences(article_text)\n",
    "    if len(sentences) < 2:\n",
    "        return [], []\n",
    "    \n",
    "    summary_tokens = preprocessor.text_to_indices(summary_text)\n",
    "    similarities = []\n",
    "    sentence_indices = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sent_tokens = preprocessor.text_to_indices(sentence)\n",
    "        similarity = compute_sentence_similarity(sent_tokens, summary_tokens)\n",
    "        similarities.append(similarity)\n",
    "        sentence_indices.append(sent_tokens)\n",
    "    \n",
    "    if not similarities:\n",
    "        return [], []\n",
    "    \n",
    "    similarities = np.array(similarities)\n",
    "    num_to_select = max(1, int(len(sentences) * top_ratio))\n",
    "    top_indices = np.argsort(similarities)[-num_to_select:]\n",
    "    \n",
    "    labels = np.zeros(len(sentences), dtype=int)\n",
    "    labels[top_indices] = 1\n",
    "    \n",
    "    return sentence_indices, labels\n",
    "\n",
    "class ImprovedRNNEncoder:\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, dropout=0.1):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Initialize parameters with Xavier initialization\n",
    "        self.embedding = np.random.normal(0, np.sqrt(2.0/embed_dim), \n",
    "                                        (vocab_size, embed_dim)).astype(np.float32)\n",
    "        self.W_ih = np.random.normal(0, np.sqrt(2.0/(embed_dim + hidden_dim)), \n",
    "                                   (hidden_dim, embed_dim)).astype(np.float32)\n",
    "        self.W_hh = np.random.normal(0, np.sqrt(2.0/hidden_dim), \n",
    "                                   (hidden_dim, hidden_dim)).astype(np.float32)\n",
    "        self.b_h = np.zeros(hidden_dim).astype(np.float32)\n",
    "        \n",
    "        # Store gradients\n",
    "        self.grad_embedding = np.zeros_like(self.embedding)\n",
    "        self.grad_W_ih = np.zeros_like(self.W_ih)\n",
    "        self.grad_W_hh = np.zeros_like(self.W_hh)\n",
    "        self.grad_b_h = np.zeros_like(self.b_h)\n",
    "\n",
    "    def forward(self, sequence, training=True):\n",
    "        \"\"\"Forward pass with improved dropout - FIXED to always return 4 values\"\"\"\n",
    "        if len(sequence) == 0:\n",
    "            return np.zeros(self.hidden_dim).astype(np.float32), [], [], []\n",
    "        \n",
    "        h = np.zeros(self.hidden_dim).astype(np.float32)\n",
    "        hidden_states = []\n",
    "        embeddings = []\n",
    "        \n",
    "        for token_idx in sequence:\n",
    "            if token_idx >= self.vocab_size:\n",
    "                token_idx = 1  # UNK token\n",
    "                \n",
    "            x = self.embedding[token_idx].copy()\n",
    "            \n",
    "            # Apply dropout to input\n",
    "            if training and self.dropout > 0:\n",
    "                dropout_mask = np.random.binomial(1, 1-self.dropout, x.shape).astype(np.float32)\n",
    "                x = x * dropout_mask / (1-self.dropout)\n",
    "            \n",
    "            embeddings.append(x)\n",
    "            \n",
    "            # RNN computation\n",
    "            h_new = np.tanh(np.dot(self.W_ih, x) + np.dot(self.W_hh, h) + self.b_h)\n",
    "            \n",
    "            # Apply dropout to hidden state\n",
    "            if training and self.dropout > 0:\n",
    "                h_dropout_mask = np.random.binomial(1, 1-self.dropout, h_new.shape).astype(np.float32)\n",
    "                h_new = h_new * h_dropout_mask / (1-self.dropout)\n",
    "            \n",
    "            h = h_new\n",
    "            hidden_states.append(h.copy())\n",
    "        \n",
    "        return h, hidden_states, embeddings, sequence\n",
    "\n",
    "    def backward(self, grad_output, hidden_states, embeddings, sequence, learning_rate=0.001):\n",
    "        \"\"\"Backward pass with proper BPTT\"\"\"\n",
    "        if len(sequence) == 0:\n",
    "            return\n",
    "            \n",
    "        # Reset gradients\n",
    "        self.grad_embedding.fill(0)\n",
    "        self.grad_W_ih.fill(0)\n",
    "        self.grad_W_hh.fill(0)\n",
    "        self.grad_b_h.fill(0)\n",
    "        \n",
    "        grad_h = grad_output.copy()\n",
    "        \n",
    "        # Backward through time\n",
    "        for t in reversed(range(len(sequence))):\n",
    "            token_idx = sequence[t]\n",
    "            if token_idx >= self.vocab_size:\n",
    "                token_idx = 1\n",
    "                \n",
    "            h_prev = hidden_states[t-1] if t > 0 else np.zeros(self.hidden_dim)\n",
    "            x = embeddings[t]\n",
    "            \n",
    "            # Gradient through tanh\n",
    "            grad_h_raw = grad_h * (1 - hidden_states[t]**2)\n",
    "            \n",
    "            # Gradients for parameters\n",
    "            self.grad_W_ih += np.outer(grad_h_raw, x)\n",
    "            self.grad_W_hh += np.outer(grad_h_raw, h_prev)\n",
    "            self.grad_b_h += grad_h_raw\n",
    "            \n",
    "            # Gradient for embedding\n",
    "            grad_x = np.dot(self.W_ih.T, grad_h_raw)\n",
    "            self.grad_embedding[token_idx] += grad_x\n",
    "            \n",
    "            # Gradient for previous hidden state\n",
    "            if t > 0:\n",
    "                grad_h = np.dot(self.W_hh.T, grad_h_raw)\n",
    "        \n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        max_norm = 5.0\n",
    "        self.grad_embedding = np.clip(self.grad_embedding, -max_norm, max_norm)\n",
    "        self.grad_W_ih = np.clip(self.grad_W_ih, -max_norm, max_norm)\n",
    "        self.grad_W_hh = np.clip(self.grad_W_hh, -max_norm, max_norm)\n",
    "        self.grad_b_h = np.clip(self.grad_b_h, -max_norm, max_norm)\n",
    "        \n",
    "        # Update parameters\n",
    "        self.embedding -= learning_rate * self.grad_embedding\n",
    "        self.W_ih -= learning_rate * self.grad_W_ih\n",
    "        self.W_hh -= learning_rate * self.grad_W_hh\n",
    "        self.b_h -= learning_rate * self.grad_b_h\n",
    "\n",
    "class ImprovedSentenceEncoder:\n",
    "    def __init__(self, word_encoder):\n",
    "        self.word_encoder = word_encoder\n",
    "        hidden_dim = word_encoder.hidden_dim\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.W_ih_sent = np.random.normal(0, np.sqrt(2.0/hidden_dim), \n",
    "                                        (hidden_dim, hidden_dim)).astype(np.float32)\n",
    "        self.W_hh_sent = np.random.normal(0, np.sqrt(2.0/hidden_dim), \n",
    "                                        (hidden_dim, hidden_dim)).astype(np.float32)\n",
    "        self.b_h_sent = np.zeros(hidden_dim).astype(np.float32)\n",
    "        \n",
    "        # Store gradients\n",
    "        self.grad_W_ih_sent = np.zeros_like(self.W_ih_sent)\n",
    "        self.grad_W_hh_sent = np.zeros_like(self.W_hh_sent)\n",
    "        self.grad_b_h_sent = np.zeros_like(self.b_h_sent)\n",
    "\n",
    "    def forward(self, sentences, training=True):\n",
    "        \"\"\"Encode document as sequence of sentences\"\"\"\n",
    "        if not sentences:\n",
    "            return [], []\n",
    "        \n",
    "        sentence_representations = []\n",
    "        sentence_data = []  # Store data needed for backprop\n",
    "        \n",
    "        # Encode each sentence\n",
    "        for sentence in sentences:\n",
    "            sent_rep, hidden_states, embeddings, sequence = self.word_encoder.forward(sentence, training)\n",
    "            sentence_representations.append(sent_rep)\n",
    "            sentence_data.append((hidden_states, embeddings, sequence))\n",
    "        \n",
    "        # Document-level RNN\n",
    "        h_doc = np.zeros(self.word_encoder.hidden_dim).astype(np.float32)\n",
    "        contextual_sentence_reps = []\n",
    "        doc_hidden_states = []\n",
    "        \n",
    "        for sent_rep in sentence_representations:\n",
    "            h_doc = np.tanh(np.dot(self.W_ih_sent, sent_rep) + \n",
    "                          np.dot(self.W_hh_sent, h_doc) + self.b_h_sent)\n",
    "            \n",
    "            if training and self.word_encoder.dropout > 0:\n",
    "                dropout_mask = np.random.binomial(1, 1-self.word_encoder.dropout, h_doc.shape)\n",
    "                h_doc = h_doc * dropout_mask / (1-self.word_encoder.dropout)\n",
    "            \n",
    "            contextual_sentence_reps.append(h_doc.copy())\n",
    "            doc_hidden_states.append(h_doc.copy())\n",
    "        \n",
    "        return contextual_sentence_reps, (sentence_data, doc_hidden_states, sentence_representations)\n",
    "\n",
    "    def backward(self, grad_outputs, forward_data, learning_rate=0.001):\n",
    "        \"\"\"Backward pass through sentence encoder\"\"\"\n",
    "        sentence_data, doc_hidden_states, sentence_representations = forward_data\n",
    "        \n",
    "        # Reset gradients\n",
    "        self.grad_W_ih_sent.fill(0)\n",
    "        self.grad_W_hh_sent.fill(0)\n",
    "        self.grad_b_h_sent.fill(0)\n",
    "        \n",
    "        grad_h_doc = np.zeros(self.word_encoder.hidden_dim)\n",
    "        \n",
    "        # Backward through document-level RNN\n",
    "        for t in reversed(range(len(grad_outputs))):\n",
    "            grad_h_raw = (grad_outputs[t] + grad_h_doc) * (1 - doc_hidden_states[t]**2)\n",
    "            \n",
    "            h_prev = doc_hidden_states[t-1] if t > 0 else np.zeros(self.word_encoder.hidden_dim)\n",
    "            sent_rep = sentence_representations[t]\n",
    "            \n",
    "            # Update gradients\n",
    "            self.grad_W_ih_sent += np.outer(grad_h_raw, sent_rep)\n",
    "            self.grad_W_hh_sent += np.outer(grad_h_raw, h_prev)\n",
    "            self.grad_b_h_sent += grad_h_raw\n",
    "            \n",
    "            # Gradient for sentence representation\n",
    "            grad_sent_rep = np.dot(self.W_ih_sent.T, grad_h_raw)\n",
    "            \n",
    "            # Backpropagate to word encoder\n",
    "            hidden_states, embeddings, sequence = sentence_data[t]\n",
    "            self.word_encoder.backward(grad_sent_rep, hidden_states, embeddings, sequence, learning_rate)\n",
    "            \n",
    "            # Gradient for previous doc hidden state\n",
    "            if t > 0:\n",
    "                grad_h_doc = np.dot(self.W_hh_sent.T, grad_h_raw)\n",
    "        \n",
    "        # Clip and update parameters\n",
    "        max_norm = 5.0\n",
    "        self.grad_W_ih_sent = np.clip(self.grad_W_ih_sent, -max_norm, max_norm)\n",
    "        self.grad_W_hh_sent = np.clip(self.grad_W_hh_sent, -max_norm, max_norm)\n",
    "        self.grad_b_h_sent = np.clip(self.grad_b_h_sent, -max_norm, max_norm)\n",
    "        \n",
    "        self.W_ih_sent -= learning_rate * self.grad_W_ih_sent\n",
    "        self.W_hh_sent -= learning_rate * self.grad_W_hh_sent\n",
    "        self.b_h_sent -= learning_rate * self.grad_b_h_sent\n",
    "\n",
    "class ImprovedBinaryClassifier:\n",
    "    def __init__(self, input_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.W_class = np.random.normal(0, np.sqrt(2.0/input_dim), (1, input_dim)).astype(np.float32)\n",
    "        self.b_class = np.zeros(1).astype(np.float32)\n",
    "\n",
    "    def forward(self, representations):\n",
    "        \"\"\"Binary classification for each sentence\"\"\"\n",
    "        if not representations:\n",
    "            return []\n",
    "        \n",
    "        probabilities = []\n",
    "        for rep in representations:\n",
    "            logit = np.dot(self.W_class, rep) + self.b_class\n",
    "            prob = 1.0 / (1.0 + np.exp(-np.clip(logit, -10, 10)))\n",
    "            probabilities.append(prob[0])\n",
    "        \n",
    "        return np.array(probabilities)\n",
    "\n",
    "    def backward(self, grad_outputs, representations, learning_rate=0.001):\n",
    "        \"\"\"Backward pass through classifier\"\"\"\n",
    "        grad_representations = []\n",
    "        grad_W = np.zeros_like(self.W_class)\n",
    "        grad_b = np.zeros_like(self.b_class)\n",
    "        \n",
    "        for i, (grad_out, rep) in enumerate(zip(grad_outputs, representations)):\n",
    "            # Gradients for parameters\n",
    "            grad_W += grad_out * rep.reshape(1, -1)\n",
    "            grad_b += grad_out\n",
    "            \n",
    "            # Gradient for representation\n",
    "            grad_rep = grad_out * self.W_class.flatten()\n",
    "            grad_representations.append(grad_rep)\n",
    "        \n",
    "        # Clip and update\n",
    "        max_norm = 5.0\n",
    "        grad_W = np.clip(grad_W, -max_norm, max_norm)\n",
    "        grad_b = np.clip(grad_b, -max_norm, max_norm)\n",
    "        \n",
    "        self.W_class -= learning_rate * grad_W\n",
    "        self.b_class -= learning_rate * grad_b\n",
    "        \n",
    "        return grad_representations\n",
    "\n",
    "class ImprovedExtractiveRNNSummarizer:\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256):\n",
    "        self.word_encoder = ImprovedRNNEncoder(vocab_size, embed_dim, hidden_dim)\n",
    "        self.sentence_encoder = ImprovedSentenceEncoder(self.word_encoder)\n",
    "        self.classifier = ImprovedBinaryClassifier(hidden_dim)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, sentences, training=True):\n",
    "        \"\"\"Forward pass through the complete model\"\"\"\n",
    "        sentence_reps, forward_data = self.sentence_encoder.forward(sentences, training)\n",
    "        if not sentence_reps:\n",
    "            return np.array([]), None\n",
    "        \n",
    "        probabilities = self.classifier.forward(sentence_reps)\n",
    "        return probabilities, (sentence_reps, forward_data)\n",
    "\n",
    "    def backward(self, loss_gradients, forward_data, learning_rate=0.001):\n",
    "        \"\"\"Backward pass through the complete model\"\"\"\n",
    "        sentence_reps, sentence_forward_data = forward_data\n",
    "        \n",
    "        # Backprop through classifier\n",
    "        grad_sentence_reps = self.classifier.backward(loss_gradients, sentence_reps, learning_rate)\n",
    "        \n",
    "        # Backprop through sentence encoder\n",
    "        self.sentence_encoder.backward(grad_sentence_reps, sentence_forward_data, learning_rate)\n",
    "\n",
    "def compute_loss_and_gradients(model, sentences, labels, learning_rate=0.001):\n",
    "    \"\"\"Compute loss and perform full backpropagation\"\"\"\n",
    "    if not sentences or len(labels) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Forward pass\n",
    "    probs, forward_data = model.forward(sentences, training=True)\n",
    "    if len(probs) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Ensure same length\n",
    "    min_len = min(len(probs), len(labels))\n",
    "    probs = probs[:min_len]\n",
    "    labels = labels[:min_len]\n",
    "    \n",
    "    # Compute loss\n",
    "    probs = np.clip(probs, 1e-8, 1-1e-8)\n",
    "    loss = -np.mean(labels * np.log(probs) + (1 - labels) * np.log(1 - probs))\n",
    "    \n",
    "    if not np.isfinite(loss):\n",
    "        return 0.0\n",
    "    \n",
    "    # Compute gradients\n",
    "    grad_probs = (probs - labels) / len(labels)\n",
    "    \n",
    "    # Backward pass\n",
    "    model.backward(grad_probs, forward_data, learning_rate)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def evaluate_model(model, data, preprocessor, max_samples=None):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    total_loss = 0.0\n",
    "    predictions_all = []\n",
    "    labels_all = []\n",
    "    num_samples = 0\n",
    "    \n",
    "    rouge1_list, rouge2_list, rougel_list, bleu_list = [], [], [], []\n",
    "    \n",
    "    samples_to_eval = min(len(data), max_samples) if max_samples else len(data)\n",
    "    \n",
    "    for i in range(samples_to_eval):\n",
    "        article, summary = data[i]\n",
    "        sentences, labels = create_extractive_labels(article, summary, preprocessor)\n",
    "        \n",
    "        if not sentences or len(labels) == 0:\n",
    "            continue\n",
    "            \n",
    "        probs, _ = model.forward(sentences, training=False)\n",
    "        if len(probs) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Compute classification metrics\n",
    "        min_len = min(len(probs), len(labels))\n",
    "        probs_eval = np.clip(probs[:min_len], 1e-8, 1-1e-8)\n",
    "        labels_eval = labels[:min_len]\n",
    "        \n",
    "        loss = -np.mean(labels_eval * np.log(probs_eval) +\n",
    "                        (1 - labels_eval) * np.log(1 - probs_eval))\n",
    "        \n",
    "        if np.isfinite(loss):\n",
    "            total_loss += loss\n",
    "            predictions_all.extend((probs_eval > 0.5).astype(int))\n",
    "            labels_all.extend(labels_eval)\n",
    "            num_samples += 1\n",
    "        \n",
    "        # Generate summary and compute ROUGE/BLEU\n",
    "        generated_sentences = generate_summary(model, article, preprocessor)\n",
    "        hyp_tokens = preprocessor.text_to_indices(' '.join(generated_sentences))\n",
    "        ref_tokens = preprocessor.text_to_indices(summary)\n",
    "        \n",
    "        rouge1_list.append(rouge_n(hyp_tokens, ref_tokens, n=1))\n",
    "        rouge2_list.append(rouge_n(hyp_tokens, ref_tokens, n=2))\n",
    "        rougel_list.append(rouge_l(hyp_tokens, ref_tokens))\n",
    "        bleu_list.append(bleu_score(hyp_tokens, ref_tokens))\n",
    "\n",
    "    if num_samples == 0:\n",
    "        return float('inf'), 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "    \n",
    "    # Classification metrics\n",
    "    avg_loss = total_loss / num_samples\n",
    "    predictions_all = np.array(predictions_all)\n",
    "    labels_all = np.array(labels_all)\n",
    "    \n",
    "    tp = np.sum((predictions_all == 1) & (labels_all == 1))\n",
    "    fp = np.sum((predictions_all == 1) & (labels_all == 0))\n",
    "    fn = np.sum((predictions_all == 0) & (labels_all == 1))\n",
    "    tn = np.sum((predictions_all == 0) & (labels_all == 0))\n",
    "    \n",
    "    accuracy = (tp + tn) / (tp + fp + fn + tn) if (tp + fp + fn + tn) > 0 else 0.0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    # Summary quality metrics\n",
    "    rouge1 = float(np.mean(rouge1_list)) if rouge1_list else 0.0\n",
    "    rouge2 = float(np.mean(rouge2_list)) if rouge2_list else 0.0\n",
    "    rougel = float(np.mean(rougel_list)) if rougel_list else 0.0\n",
    "    bleu = float(np.mean(bleu_list)) if bleu_list else 0.0\n",
    "    \n",
    "    return avg_loss, accuracy, precision, recall, f1, rouge1, rouge2, rougel, bleu\n",
    "\n",
    "def generate_summary(model, article_text, preprocessor, max_sentences=3, threshold=0.5):\n",
    "    \"\"\"Generate extractive summary for given article\"\"\"\n",
    "    sentences_text = split_into_sentences(article_text)\n",
    "    if not sentences_text:\n",
    "        return []\n",
    "    \n",
    "    sentences_indices = []\n",
    "    for sent_text in sentences_text:\n",
    "        sent_indices = preprocessor.text_to_indices(sent_text)\n",
    "        sentences_indices.append(sent_indices)\n",
    "    \n",
    "    probs, _ = model.forward(sentences_indices, training=False)\n",
    "    if len(probs) == 0:\n",
    "        return sentences_text[:max_sentences]\n",
    "    \n",
    "    # Select sentences based on threshold\n",
    "    selected_indices = []\n",
    "    for i, prob in enumerate(probs):\n",
    "        if prob > threshold:\n",
    "            selected_indices.append(i)\n",
    "    \n",
    "    # If no sentences meet threshold, select top-scoring ones\n",
    "    if len(selected_indices) == 0:\n",
    "        top_indices = np.argsort(probs)[-max_sentences:]\n",
    "        selected_indices = sorted(top_indices)\n",
    "    \n",
    "    # Limit to max_sentences\n",
    "    if len(selected_indices) > max_sentences:\n",
    "        scored_indices = [(i, probs[i]) for i in selected_indices]\n",
    "        scored_indices.sort(key=lambda x: x[1], reverse=True)\n",
    "        selected_indices = [i for i, _ in scored_indices[:max_sentences]]\n",
    "        selected_indices.sort()\n",
    "    \n",
    "    summary_sentences = [sentences_text[i] for i in selected_indices if i < len(sentences_text)]\n",
    "    return summary_sentences\n",
    "\n",
    "def train_extractive_summarizer(train_data, val_data, preprocessor,\n",
    "                                n_epochs=10, learning_rate=0.001, patience=3):\n",
    "    \"\"\"Train the improved extractive summarizer with full backpropagation\"\"\"\n",
    "    print(\"Initializing improved model...\")\n",
    "    model = ImprovedExtractiveRNNSummarizer(vocab_size=preprocessor.vocab_size)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    print(f\"Starting training for {n_epochs} epochs with learning rate {learning_rate}...\")\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_start = time.time()\n",
    "        random.shuffle(train_data)\n",
    "        \n",
    "        total_train_loss = 0.0\n",
    "        num_train_samples = 0\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1}/{n_epochs}\")\n",
    "        print(\"Training...\")\n",
    "        \n",
    "        # Training loop\n",
    "        for i, (article, summary) in enumerate(train_data):\n",
    "            sentences, labels = create_extractive_labels(article, summary, preprocessor)\n",
    "            if not sentences or len(labels) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Compute loss and perform backpropagation\n",
    "            loss = compute_loss_and_gradients(model, sentences, labels, learning_rate)\n",
    "            if loss > 0:\n",
    "                total_train_loss += loss\n",
    "                num_train_samples += 1\n",
    "            \n",
    "            if (i + 1) % 50 == 0:  # More frequent progress updates\n",
    "                current_avg_loss = total_train_loss / max(1, num_train_samples)\n",
    "                print(f\"  Processed {i + 1}/{len(train_data)} samples, Avg Loss: {current_avg_loss:.4f}\")\n",
    "        \n",
    "        avg_train_loss = total_train_loss / max(1, num_train_samples) if num_train_samples > 0 else float('inf')\n",
    "        \n",
    "        # Validation\n",
    "        print(\"Validating...\")\n",
    "        val_loss, val_acc, val_prec, val_recall, val_f1, r1, r2, rl, bleu = evaluate_model(\n",
    "            model, val_data, preprocessor, max_samples=100\n",
    "        )\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"Epoch {epoch + 1} Results:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"  Val Accuracy: {val_acc:.3f}\")\n",
    "        print(f\"  Val Precision: {val_prec:.3f}\")\n",
    "        print(f\"  Val Recall: {val_recall:.3f}\")\n",
    "        print(f\"  Val F1-Score: {val_f1:.3f}\")\n",
    "        print(f\"  Val ROUGE-1: {r1:.3f}, ROUGE-2: {r2:.3f}, ROUGE-L: {rl:.3f}, BLEU: {bleu:.3f}\")\n",
    "        print(f\"  Time: {epoch_time:.1f}s\")\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            print(f\"  New best validation loss!\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "    \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "def save_model(model, preprocessor, filename=\"improved_rnn_model.pkl\"):\n",
    "    \"\"\"Save the trained model and preprocessor\"\"\"\n",
    "    model_data = {\n",
    "        'preprocessor': preprocessor,\n",
    "        'model_params': {\n",
    "            'word_encoder': {\n",
    "                'embedding': model.word_encoder.embedding,\n",
    "                'W_ih': model.word_encoder.W_ih,\n",
    "                'W_hh': model.word_encoder.W_hh,\n",
    "                'b_h': model.word_encoder.b_h\n",
    "            },\n",
    "            'sentence_encoder': {\n",
    "                'W_ih_sent': model.sentence_encoder.W_ih_sent,\n",
    "                'W_hh_sent': model.sentence_encoder.W_hh_sent,\n",
    "                'b_h_sent': model.sentence_encoder.b_h_sent\n",
    "            },\n",
    "            'classifier': {\n",
    "                'W_class': model.classifier.W_class,\n",
    "                'b_class': model.classifier.b_class\n",
    "            }\n",
    "        },\n",
    "        'config': {\n",
    "            'vocab_size': model.vocab_size,\n",
    "            'embed_dim': model.embed_dim,\n",
    "            'hidden_dim': model.hidden_dim\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        print(f\"Model saved to '{filename}'\")\n",
    "    except (IOError, pickle.PickleError) as e:\n",
    "        print(f\"Error saving model: {e}\")\n",
    "\n",
    "def load_model(filename=\"improved_rnn_model.pkl\"):\n",
    "    \"\"\"Load a trained model\"\"\"\n",
    "    try:\n",
    "        with open(filename, \"rb\") as f:\n",
    "            model_data = pickle.load(f)\n",
    "        \n",
    "        config = model_data['config']\n",
    "        model = ImprovedExtractiveRNNSummarizer(\n",
    "            vocab_size=config['vocab_size'],\n",
    "            embed_dim=config['embed_dim'],\n",
    "            hidden_dim=config['hidden_dim']\n",
    "        )\n",
    "        \n",
    "        # Load parameters\n",
    "        params = model_data['model_params']\n",
    "        model.word_encoder.embedding = params['word_encoder']['embedding']\n",
    "        model.word_encoder.W_ih = params['word_encoder']['W_ih']\n",
    "        model.word_encoder.W_hh = params['word_encoder']['W_hh']\n",
    "        model.word_encoder.b_h = params['word_encoder']['b_h']\n",
    "        \n",
    "        model.sentence_encoder.W_ih_sent = params['sentence_encoder']['W_ih_sent']\n",
    "        model.sentence_encoder.W_hh_sent = params['sentence_encoder']['W_hh_sent']\n",
    "        model.sentence_encoder.b_h_sent = params['sentence_encoder']['b_h_sent']\n",
    "        \n",
    "        model.classifier.W_class = params['classifier']['W_class']\n",
    "        model.classifier.b_class = params['classifier']['b_class']\n",
    "        \n",
    "        preprocessor = model_data['preprocessor']\n",
    "        \n",
    "        print(f\"Model loaded from '{filename}'\")\n",
    "        return model, preprocessor\n",
    "        \n",
    "    except (IOError, pickle.PickleError) as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Setting up improved extractive summarizer...\")\n",
    "    \n",
    "    # Prepare data\n",
    "    train_data = list(zip(train_src, train_tgt))\n",
    "    val_data = list(zip(val_src, val_tgt))\n",
    "    test_data = list(zip(test_src, test_tgt))\n",
    "    \n",
    "    print(f\"Train samples: {len(train_data)}\")\n",
    "    print(f\"Val samples: {len(val_data)}\")\n",
    "    print(f\"Test samples: {len(test_data)}\")\n",
    "    \n",
    "    # Build vocabulary\n",
    "    preprocessor = TextPreprocessor(vocab_size=5000)  # Smaller vocab for demo\n",
    "    all_texts = train_src + train_tgt + val_src + val_tgt\n",
    "    preprocessor.build_vocabulary(all_texts)\n",
    "    \n",
    "    # Example preprocessing\n",
    "    print(\"\\nExample preprocessing:\")\n",
    "    if len(train_src) > 0:\n",
    "        sample_text = train_src[0]\n",
    "        print(f\"Original: {sample_text[:100]}...\")\n",
    "        tokens = preprocessor.text_to_indices(sample_text)\n",
    "        print(f\"Tokenized: {tokens[:20]}...\")\n",
    "        reconstructed = preprocessor.indices_to_text(tokens[:20])\n",
    "        print(f\"Reconstructed: {reconstructed}\")\n",
    "    \n",
    "    print(f\"\\n=== Training Improved Extractive Summarizer ===\")\n",
    "    \n",
    "    # Use smaller subsets for demonstration\n",
    "    train_subset = train_data[:len(train_data)] if len(train_data) <= 1000 else train_data[:1000]\n",
    "    val_subset = val_data[:len(val_data)] if len(val_data) <= 200 else val_data[:200]\n",
    "    \n",
    "    # Train the model\n",
    "    model, train_losses, val_losses = train_extractive_summarizer(\n",
    "        train_subset, val_subset, preprocessor,\n",
    "        n_epochs=5, learning_rate=0.01, patience=3\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n=== Testing ===\")\n",
    "    test_subset = test_data[:len(test_data)] if len(test_data) <= 200 else test_data[:200]\n",
    "    \n",
    "    test_loss, test_acc, test_prec, test_recall, test_f1, r1, r2, rl, bleu = evaluate_model(\n",
    "        model, test_subset, preprocessor, max_samples=len(test_subset)\n",
    "    )\n",
    "    \n",
    "    print(f\"Final Test Results:\")\n",
    "    print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"  Test Accuracy: {test_acc:.3f}\")\n",
    "    print(f\"  Test Precision: {test_prec:.3f}\")\n",
    "    print(f\"  Test Recall: {test_recall:.3f}\")\n",
    "    print(f\"  Test F1-Score: {test_f1:.3f}\")\n",
    "    print(f\"  Test ROUGE-1: {r1:.3f}, ROUGE-2: {r2:.3f}, ROUGE-L: {rl:.3f}, BLEU: {bleu:.3f}\")\n",
    "    \n",
    "    print(f\"\\n=== Sample Summaries ===\")\n",
    "    for i in range(min(3, len(test_data))):\n",
    "        article = test_data[i][0]\n",
    "        reference = test_data[i][1]\n",
    "        \n",
    "        print(f\"\\nExample {i + 1}:\")\n",
    "        print(f\"Article: {article[:200]}...\")\n",
    "        print(f\"Reference: {reference}\")\n",
    "        \n",
    "        generated_sentences = generate_summary(model, article, preprocessor, max_sentences=2)\n",
    "        generated_summary = ' '.join(generated_sentences)\n",
    "        print(f\"Generated: {generated_summary}\")\n",
    "        \n",
    "        # Compute individual ROUGE scores for this example\n",
    "        hyp_tokens = preprocessor.text_to_indices(generated_summary)\n",
    "        ref_tokens = preprocessor.text_to_indices(reference)\n",
    "        \n",
    "        r1_score = rouge_n(hyp_tokens, ref_tokens, n=1)\n",
    "        r2_score = rouge_n(hyp_tokens, ref_tokens, n=2)\n",
    "        rl_score = rouge_l(hyp_tokens, ref_tokens)\n",
    "        bleu_score_val = bleu_score(hyp_tokens, ref_tokens)\n",
    "        \n",
    "        print(f\"Scores - R1: {r1_score:.3f}, R2: {r2_score:.3f}, RL: {rl_score:.3f}, BLEU: {bleu_score_val:.3f}\")\n",
    "    \n",
    "    # Save results\n",
    "    results = {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'test_metrics': {\n",
    "            'loss': test_loss,\n",
    "            'accuracy': test_acc,\n",
    "            'precision': test_prec,\n",
    "            'recall': test_recall,\n",
    "            'f1': test_f1,\n",
    "            'rouge1': r1,\n",
    "            'rouge2': r2,\n",
    "            'rougel': rl,\n",
    "            'bleu': bleu\n",
    "        },\n",
    "        'vocab_size': preprocessor.vocab_size,\n",
    "        'model_config': {\n",
    "            'embed_dim': model.embed_dim,\n",
    "            'hidden_dim': model.hidden_dim\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with open(\"improved_rnn_results.pkl\", \"wb\") as f:\n",
    "            pickle.dump(results, f)\n",
    "        print(f\"\\nResults saved to 'improved_rnn_results.pkl'\")\n",
    "    except (IOError, pickle.PickleError) as e:\n",
    "        print(f\"Error saving results: {e}\")\n",
    "    \n",
    "    # Save the trained model\n",
    "    save_model(model, preprocessor, \"improved_rnn_model.pkl\")\n",
    "    \n",
    "    print(f\"\\n=== Training Summary ===\")\n",
    "    print(f\"Best validation loss: {min(val_losses):.4f}\")\n",
    "    print(f\"Final test F1-score: {test_f1:.3f}\")\n",
    "    print(f\"Final test ROUGE-1: {r1:.3f}\")\n",
    "    print(\"Training completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notesyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
